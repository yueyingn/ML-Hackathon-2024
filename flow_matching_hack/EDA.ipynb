{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ecb9f-28af-4bdd-8c0d-7de5bc634122",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from glob import glob\n",
    "import h5py\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional \n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import Resize as VisionResize\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../models')\n",
    "\n",
    "from unet import ResnetBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109e299-eca0-4520-bf1c-63bbb4c3dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/ocean/projects/phy230064p/shared/superresolution/'\n",
    "simPath = basePath + '/Maps_Mcdm_IllustrisTNG_CV_z=0.00.npy'\n",
    "dm_map = np.load(simPath)\n",
    "\n",
    "# density  normalize the data\n",
    "dm_map = np.log10(dm_map)\n",
    "dm_map = (dm_map - dm_map.mean()) / dm_map.std()\n",
    "print (\"shape of the data:\",dm_map.shape)\n",
    "\n",
    "dm_map = torch.Tensor(dm_map)\n",
    "dm_map = dm_map.unsqueeze(1) # (N,H,W) -> (N,C,H,W)\n",
    "print (\"shape of the data:\",dm_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414031b5-fabb-44df-91cb-ea65ad64f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate(object):\n",
    "    \"\"\"\n",
    "    Apply translation to the input image\n",
    "    \"\"\"\n",
    "    def __init__(self, ndim):\n",
    "        self.ndim = ndim\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        in_img = sample \n",
    "        \n",
    "        shift_dims = tuple(np.arange(self.ndim)-self.ndim)\n",
    "        shift_pixels = tuple([torch.randint(in_img.shape[d], (1,)).item() for d in shift_dims])\n",
    "\n",
    "        in_img = torch.roll(in_img, shift_pixels, dims=shift_dims)\n",
    "        \n",
    "        return in_img\n",
    "    \n",
    "\n",
    "class Flip(object):\n",
    "    \"\"\"\n",
    "    Flip the input images \n",
    "    \"\"\"\n",
    "    def __init__(self, ndim):\n",
    "        self.axes = None\n",
    "        self.ndim = ndim\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        assert self.ndim > 1, \"flipping is ambiguous for 1D scalars/vectors\"\n",
    "\n",
    "        self.axes = torch.randint(2, (self.ndim,), dtype=torch.bool)\n",
    "        self.axes = torch.arange(self.ndim)[self.axes]\n",
    "\n",
    "        in_img = sample\n",
    "\n",
    "        if in_img.shape[0] == self.ndim:  # flip vector components\n",
    "            in_img[self.axes] = -in_img[self.axes]\n",
    "\n",
    "        shifted_axes = (1 + self.axes).tolist()\n",
    "        in_img = torch.flip(in_img, shifted_axes)\n",
    "\n",
    "        return in_img\n",
    "    \n",
    "\n",
    "class Permutate(object):\n",
    "    \"\"\"\n",
    "    Permutate the input images \n",
    "    \"\"\"\n",
    "    def __init__(self, ndim):\n",
    "        self.axes = None\n",
    "        self.ndim = ndim\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        assert self.ndim > 1, \"permutation is not necessary for 1D fields\"\n",
    "\n",
    "        self.axes = torch.randperm(self.ndim)\n",
    "        \n",
    "        in_img = sample\n",
    "\n",
    "        if in_img.shape[0] == self.ndim:  # permutate vector components\n",
    "            in_img = in_img[self.axes]\n",
    "\n",
    "        shifted_axes = [0] + (1 + self.axes).tolist()\n",
    "        in_img = in_img.permute(shifted_axes)\n",
    "\n",
    "        return in_img\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f8df3-7064-4f2c-b569-90afc40df1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupResDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Make the pairs of LR, HR img pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs, lr_size, hr_size, transform=None):\n",
    "        \"\"\"\n",
    "        imgs: original img in shape of (N,C,H,W)\n",
    "        lr_size: size of low resolution\n",
    "        hr_size: size of high resolution\n",
    "        transform: operations of data augmentation, before resizing to HR and LR\n",
    "        \"\"\"\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.lr_resize = VisionResize(lr_size, antialias=False)\n",
    "        self.hr_resize = VisionResize(hr_size, antialias=False)\n",
    "        self.hr_interpolate = lambda x: torch.nn.functional.interpolate(\n",
    "            x, \n",
    "            scale_factor=hr_size / lr_size,\n",
    "            mode='bilinear', align_corners=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        LR_img = self.hr_interpolate(self.lr_resize(img)[None]).squeeze(0)\n",
    "        HR_img = self.hr_resize(img)\n",
    "            \n",
    "        return LR_img, HR_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d385135-471a-421f-831d-ebf295ff9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "# translation + rotation of the image\n",
    "train_transforms = transforms.Compose([Translate(ndim),Flip(ndim),Permutate(ndim)]) \n",
    "\n",
    "# create the dataset\n",
    "dataset = SupResDataset(dm_map, lr_size=32, hr_size=128, transform=train_transforms)\n",
    "\n",
    "# split dataset for training and validation\n",
    "train_set_size = int(len(dataset) * 0.9)\n",
    "val_set_size = len(dataset) - train_set_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_set_size, val_set_size])\n",
    "\n",
    "# Create train and validation data loaders\n",
    "BATCH_SIZE = 4\n",
    "train_loader = DataLoader(train_dataset,shuffle=True,batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fc26a-ce68-440b-856e-dfc36dc2ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(train_loader)\n",
    "for n in range(3):\n",
    "    lr_img, hr_img = next(iter_train)\n",
    "    print (lr_img.shape,hr_img.shape)\n",
    "    fig, ax = plt.subplots(ncols=2,figsize=(8,8))\n",
    "    ax[0].imshow(lr_img[0].squeeze())\n",
    "    ax[0].set_title('LR')\n",
    "    ax[1].imshow(hr_img[0].squeeze())\n",
    "    ax[1].set_title('HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3809a-d31e-400e-9a05-ceb8f53239aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.core import NeuralODE\n",
    "from lightning import LightningModule, Trainer\n",
    "\n",
    "class torch_wrapper(torch.nn.Module):\n",
    "    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, t, x, *args, **kwargs):\n",
    "        return self.model(t, x)\n",
    "\n",
    "class FlowMatching(torch.nn.Module):\n",
    "    def __init__(self, velocity_model):\n",
    "        super().__init__()\n",
    "        self.velocity_model = velocity_model\n",
    "        \n",
    "    def get_mu_t(self, x0, x1, t):\n",
    "        return t * x1 + (1 - t) * x0\n",
    "    \n",
    "    def get_gamma_t(self,t):\n",
    "        return torch.sqrt(2*t*1-t)\n",
    "    \n",
    "    def sample_xt(self, x0, x1, t, epsilon):\n",
    "        t = t.view(t.shape[0], *([1] * (x0.dim() - 1)))\n",
    "        mu_t = self.get_mu_t(x0, x1, t)\n",
    "        sigma_t = self.get_gamma_t(t)\n",
    "        return mu_t + sigma_t * epsilon\n",
    "\n",
    "    def compute_loss(self, x0, x1, t=None,):\n",
    "        if t is None:\n",
    "            t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "        eps = torch.randn_like(x0)\n",
    "        xt = self.sample_xt(x0, x1, t, eps)\n",
    "        ut = x1 - x0\n",
    "        # should condition on x0 too?\n",
    "        vt = self.velocity_model(t, xt)\n",
    "        return torch.mean((vt - ut)**2)\n",
    "    \n",
    "    def sample(self, x0, n_sampling_steps=10,):\n",
    "        node = NeuralODE(\n",
    "            torch_wrapper(model),\n",
    "            solver=\"dopri5\",\n",
    "            sensitivity=\"adjoint\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                x0,\n",
    "                t_span=torch.linspace(0, 1, n_sampling_steps),\n",
    "            )\n",
    "        return traj[-1]\n",
    "\n",
    "class LightningFlowMatching(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        velocity_model,\n",
    "        learning_rate = 1.e-4,\n",
    "        n_sampling_steps = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters('learning_rate', 'n_sampling_steps')\n",
    "        self.fm = FlowMatching(\n",
    "            velocity_model = velocity_model,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == 'fit':\n",
    "            self.fm = self.fm.to(self.device)\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        low_res, high_res = batch\n",
    "        loss = self.fm.compute_loss(low_res, high_res)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # we are going to generate some example images while training the model to check that it is generating\n",
    "        # sensible things\n",
    "        low_res, high_res = batch\n",
    "        if batch_idx == 0:\n",
    "            wandb_logger = self.logger.experiment\n",
    "            sampled_images = self.fm.sample(low_res, n_sampling_steps=self.hparams.n_sampling_steps)\n",
    "            fig, ax = plt.subplots(ncols=3,figsize=(10,3))\n",
    "            labels = ['LR','HR','SR (sample)']\n",
    "            for i,ds in enumerate([lr_img[0], hr_img[0], sampled_images[0]]):\n",
    "                x = ds.squeeze().cpu().numpy()\n",
    "                x = np.moveaxis(x,0,-1)\n",
    "                ax[i].imshow(x)\n",
    "                ax[i].set_title(labels[i],fontsize=15)\n",
    "            wandb_logger.log({\"images\": plt})\n",
    "            plt.close()\n",
    "        loss = self.fm.compute_loss(low_res, high_res)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self,):\n",
    "        optimizer = torch.optim.Adam(self.fm.parameters(), lr=self.hparams.learning_rate, betas = (0.9,0.99))\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07577b6-831a-4440-ac82-dc358618ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def get_timestep_embedding(\n",
    "    timesteps,\n",
    "    embedding_dim: int,\n",
    "    dtype=torch.float32,\n",
    "    max_timescale=10000,\n",
    "    min_timescale=1,\n",
    "):\n",
    "    # Scale timesteps by a factor of 1000\n",
    "    timesteps *= 1000\n",
    "    # Ensure timesteps is a 1-dimensional tensor\n",
    "    assert timesteps.ndim == 1\n",
    "    assert embedding_dim % 2 == 0\n",
    "\n",
    "    num_timescales = embedding_dim // 2\n",
    "    # Create a tensor of inverse timescales logarithmically spaced\n",
    "    inv_timescales = torch.logspace(\n",
    "        -np.log10(min_timescale),\n",
    "        -np.log10(max_timescale),\n",
    "        num_timescales,\n",
    "        device=timesteps.device,\n",
    "    )\n",
    "    \n",
    "    emb = timesteps.to(dtype)[:, None] * inv_timescales[None, :]  # Shape: (T, D/2)\n",
    "    # Return the concatenation of sine and cosine of the embedding\n",
    "    return torch.cat([emb.sin(), emb.cos()], dim=1)  # Shape: (T, D)\n",
    "\n",
    "class UNetVDM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_factor: int = 1,\n",
    "        embedding_dim: int = 128,\n",
    "        norm_groups: int = 32,\n",
    "        dropout_prob: float = 0.1,\n",
    "        input_channels: int = 1,\n",
    "        conditioning_channels: int = 1,\n",
    "        n_blocks: int = 32,\n",
    "        gamma_min: float = -13.3,\n",
    "        gamma_max: float = 13.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.gamma_min = gamma_min\n",
    "        self.gamma_max = gamma_max\n",
    "        \n",
    "        resnet_params = dict(\n",
    "            ch_in=embedding_dim,\n",
    "            ch_out=embedding_dim,\n",
    "            condition_dim=4 * embedding_dim,\n",
    "            dropout_prob=dropout_prob,\n",
    "            norm_groups=norm_groups,\n",
    "        )\n",
    "        \n",
    "        self.embed_conditioning = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim * 4),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        \n",
    "        total_input_ch = input_channels + conditioning_channels\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(total_input_ch, embedding_dim, 3, padding=1)\n",
    "        \n",
    "        # Down path: n_blocks blocks with a resnet block and maybe attention.\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            ResnetBlock(**resnet_params)\n",
    "            for _ in range(n_blocks)\n",
    "        )\n",
    "\n",
    "        self.mid_resnet_block_1 = ResnetBlock(**resnet_params)\n",
    "        self.mid_resnet_block_2 = ResnetBlock(**resnet_params)\n",
    "\n",
    "        # Up path: n_blocks+1 blocks with a resnet block and maybe attention.\n",
    "        resnet_params[\"ch_in\"] *= 2  # double input channels due to skip connections\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            ResnetBlock(**resnet_params)\n",
    "            for _ in range(n_blocks + 1)\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=norm_groups, num_channels=embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            zero_init(nn.Conv2d(embedding_dim, input_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g_t,\n",
    "        z,\n",
    "        conditioning=None,\n",
    "    ):\n",
    "        \n",
    "        if conditioning is not None:\n",
    "            if self.scale_factor>1:\n",
    "                conditioning = torch.nn.functional.interpolate(conditioning, scale_factor=self.scale_factor,\n",
    "                                             mode='bilinear', align_corners=False)\n",
    "            z_concat = torch.concat(\n",
    "                (z, conditioning),\n",
    "                axis=1,\n",
    "            )\n",
    "        else:\n",
    "            z_concat = z\n",
    "        \n",
    "        # Get gamma to shape (B, ).\n",
    "        g_t = g_t.expand(z_concat.shape[0])  # assume shape () or (1,) or (B,)\n",
    "        assert g_t.shape == (z_concat.shape[0],)\n",
    "        # Rescale to [0, 1], but only approximately since gamma0 & gamma1 are not fixed.\n",
    "        g_t = (g_t - self.gamma_min) / (self.gamma_max - self.gamma_min)\n",
    "        t_embedding = get_timestep_embedding(g_t, self.embedding_dim)\n",
    "        # We will condition on time embedding.\n",
    "        cond = self.embed_conditioning(t_embedding)\n",
    "        h = self.conv_in(z_concat)  # (B, embedding_dim, H, W)\n",
    "        hs = []\n",
    "        for down_block in self.down_blocks:  # n_blocks times\n",
    "            hs.append(h)\n",
    "            h = down_block(h, cond)\n",
    "        hs.append(h)\n",
    "        h = self.mid_resnet_block_1(h, cond)\n",
    "        h = self.mid_resnet_block_2(h, cond)\n",
    "        for up_block in self.up_blocks:  # n_blocks+1 times\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = up_block(h, cond)\n",
    "        prediction = self.conv_out(h)\n",
    "        return prediction + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6c162-23a4-48b6-be5b-61f18d6fa511",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_init(module: torch.nn.Module) -> torch.nn.Module:\n",
    "    \"\"\"Sets to zero all the parameters of a module, and returns the module.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        torch.nn.init.zeros_(p.data)\n",
    "    return module\n",
    "\n",
    "model = UNetVDM(\n",
    "    scale_factor = 4, # our SR task upscale the spatial resolution by 4 times\n",
    "    embedding_dim=48, \n",
    "    norm_groups = 8,\n",
    "    n_blocks = 4,\n",
    "    input_channels = 1, # For galaxy images with 3 channels (RGB)\n",
    "    conditioning_channels = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8501d-53b2-47d8-8acb-94de94c847dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_matching = LightningFlowMatching(\n",
    "    velocity_model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e22ca-e3ee-4564-9959-5602c1b92793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"IAIFI_HACK\", log_model=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8f775-3c75-4c87-836f-34a266aa5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_steps = 300_000, gradient_clip_val=0.5, logger=wandb_logger, log_every_n_steps=10, val_check_interval=20\n",
    ")\n",
    "trainer.fit(\n",
    "    model=flow_matching, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders = val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23172e0c-3d0d-4ecc-b6ad-e87b79fa870a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084dc0c-e76e-4b6e-89e0-19eb06dd3d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba825e83-020a-4ef2-b7b5-2e804a6b7cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40de428-2b74-4ded-a134-d6ed12e28c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
